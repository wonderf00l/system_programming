
![](../_resources/Pasted%20image%2020241109170111.png)

##### Регистры
![](../_resources/Pasted%20image%2020241109170133.png)
- Регистры находятся прямо на процессоре, физически занимают место на процессоре
- Их нельзя заменить/обновить
- Это энергозависимая память, при рестарте данные не сохраняются
- тут нет никакой виртуализации памяти, это буквально ячейки памяти, в которые процессор пишет, из которых читает
- У каждого регистра есть свое название
- Скорость доступа - один такт процессора



![](../_resources/Pasted%20image%2020241109170540.png)


![](../_resources/Pasted%20image%2020241109174149.png)
- В процессор вшиты массивы битов - как раз регистры
- Можно адресовывать все биты или часть массива
- Технически каждый бит представлен т.н. D-Latch'ем
	- http://simplecpu.com/memory.html
	- это триггер, который позволяет сохранять переданное значение

![](../_resources/Pasted%20image%2020241109175018.png)
- Типы регистров:
	- Внутренние - их использует сам процессор под определенные нужды
	- Регистры общего назначения - для процессора все одинаковые, однако есть договоренности, какой регистр под что использовать(например, в какой-то регистр положить номер syscall'а, в другой что-то еще и тд)



##### CPU cache
![](../_resources/Pasted%20image%2020241111235116.png)
- Кэш-память хранит некую часть данных/инструкций оперативной памяти
- Кэш появился из-за возросшей разницы между скоростью работы процессора и оперативной памяти - со временем ОЗУ стала ***бутылочным горлышком***: процессор начал простаивать, пока данные доходили из озу по шине

>Процессор взаимодействует только с cpu cache, поход напрямую в ОЗУ происходит только в том случае, если адреса ячейки памяти, к которой нужно обратиться процессору, нет в cpu cache

- Кеш быстрее ОЗУ по 2-ум причинам:
	- Расположен на процессоре, физически ближе к процессору
	- Считывание байт памяти процессором происходит напрямую, не через шину данных

###### локальность кеша
![](../_resources/Pasted%20image%2020241111235735.png)
- Локальность по времени - данные, которые сейчас используются, вероятно, снова понадобятся через какое-то время
- Локальность по расположению - данные, находящиеся недалеко от текущей обрабатываемой ячейки, скорее всего, понадобятся при дальнейшей обработке


![](../_resources/Pasted%20image%2020241112000104.png)
- На 1-ом уровне память явно разделена на память с инструкциями, память с данными
	- видимо, связано с разной локальностью секций с инструкциями и данными: инструкции - в основном пространственная локальность кеша; данные - и пространственная, и по времени
	- на 1-ом уровне кеша видимо это разделение важно для перфа

![](../_resources/Pasted%20image%2020241123182453.png)
- Подобное ограничение размера кеша и разделение на уровни - видимо, опытно и исторически полученный оптимальный сетап
	- чтобы учесть trade off между "как долго идти до памяти"(как долго электрический сигнал доходит до нужных элементов) и "как долго искать в памяти"(как быстро пройдемся по транзисторам,триггерам памяти и тп)
	- ну то есть сделать один огромный L1 кеш видимо оказалось не самым эффективным вариантом 

![](../_resources/Pasted%20image%2020241123182722.png)


#### Типы организации кешей, алгоритмы чтения из кешей
![](../_resources/Pasted%20image%2020241124154015.png)
- ***Inclusive*** - кеши внешних уровней содержат данные кешей внутренних
- ***Exclusive*** - кеши разных уровней НЕ содержат общие данные
- ***NINE*** - кеши, которые могут содержать на соседних уровнях одинаковые данные(not exclusive), при этом внутренние уровни могут содержать данные, которых нет на внешних(not inclisuve)

> Сложность Inclusive/Exclusive кешей - поддержание этой самой инклюзивности/эксклюзивности - алгоритмически и аппаратно
> 	в случае ***inclusive*** - нужно синхронизировать уровни между собой, чтобы внешние уровни обязательно содержали данные внутренних
> 	в случае ***exclusive*** - та же синхронизация, только для поддержания того, чтобы данные разных уровней не пересекались
> Поэтому на практике используется NINE подход
> ![](../_resources/Pasted%20image%2020241124154713.png)


![](../_resources/Pasted%20image%2020241124154619.png)
- Если кеш уровня ***i-1*** переполняется, его данные уходят на уровень ***i***


#### запись в кеш
![](../_resources/Pasted%20image%2020241124181816.png)
- ***Write throught policy*** - запись в адрес производится в кеши всех уровней, а также в оперативную память - то есть все синхронно и крайне медленно, процессор зависит от скорости доступа к ОЗУ
- ***Write back***(используется этот вариант)
	- процессор работает только с кешем
	- запись производится в кеш, адреса, в которые записал что-то процессор(то есть они уже не из оперативной памяти), помечаются специальным dirty bit - помеченные записи не будут вытеснены из кеша без записи в озу
	- далее асинхронно такие записи вытесняются на уровни выше, пока не ***sync'ануться*** в ОЗУ уже самим кешем
	- кеш может собрать батчи таких записей и все их зафлашить в озу


![](../_resources/Pasted%20image%2020241124182607.png)
- Кеш можно представить как некую структуру данных, по ключу хранящую ***физический*** адрес(начало блока физической памяти), по значению - саму страницу(блок) физической памяти(ОЗУ)
- Адрес физический, а не виртуальный - во избежании коллизий при обращении 2-ух разных процессов к одному и тому же виртуальному адресу, тогда программы мешали бы друг другу
- Кеш оперирует кеш-линиями - этими самыми блоками фиксированного размера(обычно 64 байта)
- Алгоритм чтения из кеша примерно следующий:
	- из полученного адреса извлекаем адрес начала кеш-линии, оффсет от начала(addr & (2^6)-1)
	- по адресу начала блока находим блок физической памяти
	- делаем оффсет в рамках блока, получаем искомый адрес, пишем по нему байт, читаем байт(ы) 


![](../_resources/Pasted%20image%2020241124211840.png)
- Есть разные подходы, как физический адрес озу будет интерпретироваться кешем, то есть каким образом кусок памяти размером 64 байта(линия) отобразится в кеш(кеш-линию). то есть где будет лежать этот кусок в кеше физически

![](../_resources/Pasted%20image%2020241124212818.png)
1. Одноассоциативный кеш
	- Блок памяти озу, адрес его начала отображается в кеш ровно 1-им конкртеным образом, то есть он будет располагаться в кеше ровному по 1-ому конкретному индексу(если мыслить кеш как некий harware array) - иначе говоря, для этого физического блока ровно одна ассоциация в cpu cache
	- Алгоритмически проще, т.к. при работе с конкретным адресом в программе мы сразу можем узнать, где в кеше располагается блок памяти, в который входит этот адрес - знаем мы это исходя из address layout - 32 бита адреса являются тем самым индексом. --> при таком подходе не осуществляется full scan кеша для поиска блока, начало адреса которого мы извлекли из полного адреса ячейки 
	- Неизбежно n блоков озу мапятся в 1 блок в кеше --> чтобы их различать, используются первые 26 бит адреса
	- Очевидна проблема с частыми коллизиями и вытеснениями из кеша: 
		- если разные разрозненные блоки озу отображаются в кеш в одну линию -> если программа постоянно работает именно с такими блоками, обращается именно к таким адресам, эти блоки будут попеременно вытесняться из этой самой линии при обращении к ним, например, в цикле

![](../_resources/Pasted%20image%2020241124212838.png)
2. N-ассоциативный кеш(n ассоциаций для ram block в cpu cache)
	- подход тот же, однако тут блок основной памяти может мапиться в n блоков памяти cpu cache
	- тогда при обращении к какому-то адресу интерпретируем его согласно уже другому layout:
		1. получаем адрес начала блока(кеш-линии)
		2. проходимся по подмассиву индексов-мест, в которые данный блок мог отобразиться в кеше
	- за счет расширения мест, куда может попасть блок в кеше уменьшаем коллизии и вытеснения 

![](../_resources/Pasted%20image%2020241124212030.png)
3. Полностью ассоциативный кеш(C-ассоциативный кеш, где C - размер кеша) - (число ассоциаций для ram block - число кеш-линий)
	- Иначе говоря, любой блок озу может отобразиться в любую кеш-линию в cpu cache
	- Более простой address layout, более интуитивно
	- Но страдает энергопотребление, т.к. при получении адреса начала кеш-линии мы пойдем full scan'ом по всему кешу в поисках блока - физически это реализуется через пропускание эл. заряда через весь кеш
	- Минимум коллизий, максимум производительности, максимум энергопотребления - ***работает на L1***


#### Параллелизм на уровне инструкций(instruction level parallelism)
![](../_resources/Pasted%20image%2020241130204750.png)
- Конвейерная обработка инструкций процессором - в 5 этапов
- Инструкции одного потока исполнения(потока инструкций, иначе, просто потока-треда-tast struct'а) могут исполняться ядром процессором параллельно
- Отличается от параллелизма на уровне потоков исполнения(потоков инструкций) - когда несколько ядер процессора исполняют потоки инструкций параллельно. При этом каждое из ядер распараллеливает исполнение своего потока инструкций


![](../_resources/Pasted%20image%2020241130210455.png)
Организация кеша нескольких ядер
- Общий кеш
	- проблемы с локами при конкурентном доступе
	- проблема увеличения размера кеша
- Локальный кеш у каждого ядра
	- Проблема обеспечения ***когерентности кешей*** - синхронизированного состояния данных кешей между собой и ОЗУ

![](../_resources/Pasted%20image%2020241130210645.png)
- Протокол ***MESI*** для обеспечения синхронизированного состояния
	- MESI - возможные состояния кеш-линий
	- при загрузке линии данных из озу впервые(то есть сейчас эта линия только в одном локальном кеше) - ей присваивается состояние ***exclusive(кеш-линией владеет только один локальный кеш)***
	- при изменении такой кеш-линии --> сразу переход в ***modified***(состоянии кеш-линии изменено, не совпадает с озу, и либо этой кеш-линии вообще не было в других кешах, либо была, но теперь она невалидна. в случае инвалидации состояние этой кеш-линии в других кешах становится ***invalid(обращение к такой линии - cache miss)***)
	- при запросе на чтение modified кеш-линии другими ядрами - выгрузка изменений в ОЗУ, загрузка кеш-линии в другой кеш, изменение состояния на ***shared*** в обоих кешах
- Наиболее затратный по перфу момент в случае всех подобных алгоритмов - ***это конкурентные чтение-запись, запись-чтение данных, происходящие в разных локальных кешах разных ядер*** - требуется актуализация данных(выгрузка в ОЗУ), переключение флагов с modified на shared и т.п.
- Поэтому с точки зрения кода нужно по возможности сокращать такие сценарии, то есть сводить к только совместному чтению(все кеш-линии shared) или раздельным чтению-записи(когда в кеш-линии разных ядер загружаются разные участки озу для модификации, то есть в одном из локальных кешей кеш-линия остается в состоянии modified)

- ***Ситуации, когда происходят лишние операции обращения к ОЗУ - false sharing***
![](../_resources/Pasted%20image%2020241130213254.png)
- Разные потоки хоть и обращаются к разным данным, но эти данные попадают в одну кеш-линию
![](../_resources/Pasted%20image%2020241130213446.png)
- Явно добавляем прокладку, чтобы разбросать данные по разным кеш-линиям


#### Execution in advance, branch prediction
![](../_resources/Pasted%20image%2020241130215854.png)
- Например, по ходу исполнения процессору может встретиться инструкция, результат которой определяет, какую инструкцию выполнять далее(например, инструкция jumpif или как-то так). По сути мы можем пойти дальше , только после исполнения этой инструкции целиком
- Однако процессор делает некоторое предположение о результате этой инструкции и ставит на пайплайн исполнение след инструкции, которая матчится с его предположением
- След инструкция начинает исполняться параллельно.
	- Если процессор угадал, то она довыполниться после исполнения определяющей инструкции
	- Если нет, то пайплайн сбрасывается
- ***Причем инструкция сначала частично исполняется, а только потом проверяется правомерность инструкции***


![](../_resources/Pasted%20image%2020241130220326.png)
- Перехват сигнала segfault в обработчике сигнала

![](../_resources/Pasted%20image%2020241130220356.png)
- Использование свойства выполнения in advance и факта о более быстром доступе из кеша для получения запрещенного байта из адресного пространства ядра
	- запись запрещенного байта в userspace array в строго отделенную кеш-линию(кеш предвариельно чистится) - запись за счет execution in advance
	- определение этого байта по скорости получения доступа к нему(нужный байт был загружен в кеш) уже в обработчике segfault
- upd explanation:
	- на момент экплуатации атаки вирутальные адреса ядра были фиксированы и известны
	- суть атаки - вытянуть значение по одному из адресов адресного пространства ядра
	- заводится буфер, размером max_byte_value * padding
		- 256 - диапазон значения байта
		- 4096 - байт-прокладка, чтобы сделать разнос по гарантированно разным кеш-линиям
	- предварительно чистим кеш
	- считываем значение байта по адресу из kernel space
	- выпадет segfault, но за счет execution in advance следующая инструкция все равно будет выполнена: а в ней считываем из буфера значение по i * 4096 - но в кеш будет загружена вся кеш-линия
	- далее в segfault handler определяем номер этой кеш-линии по скорости доступа к байту в буфере - минимальный доступ будет в случае cache hit
	- тем самым определяем номер 4096-блока данных - это и есть значение байта по адресу из kernel space


![](../_resources/Pasted%20image%2020241130232929.png)
![](../_resources/Pasted%20image%2020241130232940.png)
- Нужно стараться располагать логически связанные данные последовательно в памяти, в одной кеш-линии


#### Virtual memory, virtual to physical address translation

![](../_resources/Pasted%20image%2020241201000558.png)
![](../_resources/Pasted%20image%2020241201221141.png)

- Вся память(***и виртуальная, и физическая***) делится на страницы(4-8 Кб, обычно 4 Кб)

![](../_resources/Pasted%20image%2020241201000251.png)
- Трансляция адресов из виртуальных в физические происходит на уровне железа, этим занимается MMU
- Под трансляцией подразумевается мапинг виртуальной страницы на физическую
	- Причем мапинг именно номера виртуальной страницы на номер физической - относительный оффсет остается тот же


![](../_resources/Pasted%20image%2020241201003525.png)
- Мапинг виртуальных страниц на физические нужно где-то хранить - ***page table***
- Хранить полный мапинг ***всех*** виртуальных страниц виртуального адресного пространства в таблице - точно не вариант, большая часть - это неаллоцированные дыры
- Но даже если хранить мапинг только задействованный страниц - это по-прежнему много памяти(+ этот мапинг нужно поддерживать для всех процессов)
	- слишком объемно для хранения в железе
	- слишком медленно для хранения ***только*** в оперативе и ***постоянного обращения*** в оперативу


![](../_resources/Pasted%20image%2020241201003924.png)
- Поэтому есть мапинг наиболее актуальных страниц разных процессов, который хранится в MMU -  ***TLB*** - по сути это кеш полной page table, которая уже лежит в оперативе



- При промахе TLB сам ходит в оперативку и берет нужный физический адрес из page table

![[Pasted image 20241230000237.png]]


- Задача ядра сводится к поддержке маппинга и учете его для каждого процесса, чтобы мониторить его ресурсы + изменение самой структуры, характеризующей маппинг, в случае новых аллокаций и т.д.

![](../_resources/Pasted%20image%2020241201005140.png)
- Достаем адрес начала нужной физической страницы из page table в 3 lookups на основе 3-ех префиксов виртуального адреса



![](../_resources/Pasted%20image%2020241201010508.png)
- Проблема мапинга одних и тех же таблиц виртуальных пространств разных процессов на разные физические страницы
- Решается через lookup на специальный idenfitifer(pid процесса)
- т.к. этот id всего 16 бит, а pid 32 минимум, при коллизии процессов будет очистка tlb(tlb flush)

![](../_resources/Pasted%20image%2020241201010909.png)



- Про разервированный диапазон адресов под ядро в виртуальном адресном пространстве - уже устарело из-за meltdown? теперь не мапится в пространтсва процессов и живет отдельно? 
A process "owns" the entire virtual address space here, the kernel and the user portions of it.

Its inability to peek and poke the kernel code and data is not due to different address spaces, it's due to different access rights/permissions set in the page tables. Kernel pages are set up in such a way that regular applications can't access them.

It is, however, customary to refer to the two parts of one whole thing as the kernel space and the user space and that can be confusing.

![](../_resources/Pasted%20image%2020241201225137.png)
- Функции для работы с памятью типа malloc, new - возвращают кусок виртуальной памяти размером, близким к кол-ву запрошенной памяти 

![](../_resources/Pasted%20image%2020241201225104.png)

---
https://slides.com/gerold103/sysprog_eng3